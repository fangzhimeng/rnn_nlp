/usr/lib/jvm/java-8-oracle/bin/java -Dspark.master=local[2] -Didea.launcher.port=7532 -Didea.launcher.bin.path=/home/tblee/idea-IC/bin -Dfile.encoding=UTF-8 -classpath "/usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/deploy.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jfxrt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-oracle/jre/lib/javaws.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfxswt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-oracle/jre/lib/plugin.jar:/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/media/tblee/Data/Stanford courses/Spring 2016/CME323/Project/rnn_nlp/target/scala-2.11/classes:/home/tblee/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.11.8.jar:/home/tblee/spark-1.6.1/assembly/target/scala-2.11/spark-assembly-1.6.1-hadoop2.4.0.jar:/home/tblee/idea-IC/lib/idea_rt.jar" com.intellij.rt.execution.application.AppMain org.apache.spark.shallowNN.char_RNN_dist_para
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel("INFO")
16/06/01 00:30:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/06/01 00:30:11 WARN Utils: Your hostname, tblee-UX303LB resolves to a loopback address: 127.0.1.1; using 10.0.0.6 instead (on interface wlan0)
16/06/01 00:30:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Input data has vocabulary size 66, initializing network with 1 layers each has 100 hidden units, training batch size 25
16/06/01 00:30:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
16/06/01 00:30:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
Training loss at epoch 0: 6.555634764361234
Training loss at epoch 1: 2.923428221251421
[Stage 11:=============================>                            (1 + 1) / 2]Training loss at epoch 2: 2.4059149357189935
Training loss at epoch 3: 2.2670952066089027
[Stage 13:=============================>                            (1 + 1) / 2]Training loss at epoch 4: 2.2110176297804762
[Stage 14:=============================>                            (1 + 1) / 2]Training loss at epoch 5: 2.166291019471955
[Stage 15:=============================>                            (1 + 1) / 2]Training loss at epoch 6: 2.1280091690327785
Training loss at epoch 7: 2.0977506255737985
Training loss at epoch 8: 2.0779399530795537
Training loss at epoch 9: 2.059299599719968
Training loss at epoch 10: 2.0358253326302624
[Stage 20:=============================>                            (1 + 1) / 2]Training loss at epoch 11: 2.020487266059432
[Stage 21:=============================>                            (1 + 1) / 2]Training loss at epoch 12: 2.003926460135417
Training loss at epoch 13: 1.9911123740267553
Training loss at epoch 14: 1.9741289885380422
[Stage 24:=============================>                            (1 + 1) / 2]Training loss at epoch 15: 1.9589814946947868
Training loss at epoch 16: 1.9450101183655515
[Stage 26:=============================>                            (1 + 1) / 2]Training loss at epoch 17: 1.926766167078259
[Stage 27:=============================>                            (1 + 1) / 2]Training loss at epoch 18: 1.915317684603339
Training loss at epoch 19: 1.9079201110652226
Training loss at epoch 20: 1.8967506260824183
[Stage 30:=============================>                            (1 + 1) / 2]Training loss at epoch 21: 1.892860580597147
Training loss at epoch 22: 1.8847458488708104
[Stage 32:=============================>                            (1 + 1) / 2]Training loss at epoch 23: 1.8782904683942847
Training loss at epoch 24: 1.8660550852189792
[Stage 34:=============================>                            (1 + 1) / 2]Training loss at epoch 25: 1.86123949018937
[Stage 35:=============================>                            (1 + 1) / 2]Training loss at epoch 26: 1.8557134431422628
[Stage 36:=============================>                            (1 + 1) / 2]Training loss at epoch 27: 1.8475020003398117
[Stage 37:>                                                         (0 + 2) / 2]Exception in thread "main" org.apache.spark.SparkException: Job 33 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1581)
	at org.apache.spark.SparkContext$$anonfun$stop$9.apply$mcV$sp(SparkContext.scala:1740)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1739)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.shallowNN.char_RNN_dist_para$char_RNN.fit(char_RNN_dist_para.scala:392)
	at org.apache.spark.shallowNN.char_RNN_dist_para$.main(char_RNN_dist_para.scala:450)
	at org.apache.spark.shallowNN.char_RNN_dist_para.main(char_RNN_dist_para.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
16/06/01 00:40:41 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@26e89a0c)
16/06/01 00:40:41 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(33,1464766841787,JobFailed(org.apache.spark.SparkException: Job 33 cancelled because SparkContext was shut down))

Process finished with exit code 130

