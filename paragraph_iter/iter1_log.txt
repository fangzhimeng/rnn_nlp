/usr/lib/jvm/java-8-oracle/bin/java -Dspark.master=local[2] -Didea.launcher.port=7533 -Didea.launcher.bin.path=/home/tblee/idea-IC/bin -Dfile.encoding=UTF-8 -classpath "/usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/deploy.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jfxrt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-oracle/jre/lib/javaws.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfxswt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-oracle/jre/lib/plugin.jar:/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/media/tblee/Data/Stanford courses/Spring 2016/CME323/Project/rnn_nlp/target/scala-2.11/classes:/home/tblee/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.11.8.jar:/home/tblee/spark-1.6.1/assembly/target/scala-2.11/spark-assembly-1.6.1-hadoop2.4.0.jar:/home/tblee/idea-IC/lib/idea_rt.jar" com.intellij.rt.execution.application.AppMain org.apache.spark.shallowNN.char_RNN_dist_para
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel("INFO")
16/05/31 23:41:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/05/31 23:41:30 WARN Utils: Your hostname, tblee-UX303LB resolves to a loopback address: 127.0.1.1; using 10.0.0.6 instead (on interface wlan0)
16/05/31 23:41:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Input data has vocabulary size 66, initializing network with 1 layers each has 100 hidden units, training batch size 25
[Stage 9:>                                                          (0 + 0) / 2]16/05/31 23:41:35 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
16/05/31 23:41:35 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
Training loss at epoch 0: 6.377522806220394
Training loss at epoch 1: 3.1569855804018094
Training loss at epoch 2: 2.8677645996086136
Training loss at epoch 3: 2.779653663414755
[Stage 13:=============================>                            (1 + 1) / 2]Training loss at epoch 4: 2.700938695711881
Training loss at epoch 5: 2.6358739027253297
[Stage 15:=============================>                            (1 + 1) / 2]Training loss at epoch 6: 2.641926374515429
Training loss at epoch 7: 2.6079036061343848
Training loss at epoch 8: 2.5560774573486853
Training loss at epoch 9: 2.470047126984128
[Stage 19:>                                                         (0 + 2) / 2]Training loss at epoch 10: 2.4273122834843077
[Stage 20:=============================>                            (1 + 1) / 2]Training loss at epoch 11: 2.4212845664461544
[Stage 21:>                                                         (0 + 2) / 2]Training loss at epoch 12: 2.3750138166848904
Training loss at epoch 13: 2.351654636505493
Training loss at epoch 14: 2.33936448938684
Training loss at epoch 15: 2.3117713710266625
Training loss at epoch 16: 2.2959292623520846
Training loss at epoch 17: 2.281721378368739
[Stage 27:>                                                         (0 + 2) / 2]Training loss at epoch 18: 2.269945282556794
[Stage 28:>                                                         (0 + 2) / 2]Training loss at epoch 19: 2.2558851580349053
Training loss at epoch 20: 2.2443757227919634
[Stage 30:>                                                         (0 + 2) / 2]Training loss at epoch 21: 2.231665823542127
[Stage 31:=============================>                            (1 + 1) / 2]Training loss at epoch 22: 2.2182649033893145
[Stage 32:=============================>                            (1 + 1) / 2]Training loss at epoch 23: 2.2065105580898554
Training loss at epoch 24: 2.1958243293443087
[Stage 34:>                                                         (0 + 2) / 2]Training loss at epoch 25: 2.1858822189336986
[Stage 35:>                                                         (0 + 2) / 2]Training loss at epoch 26: 2.1756990209522007
[Stage 36:>                                                         (0 + 2) / 2]Training loss at epoch 27: 2.1665004327933852
Training loss at epoch 28: 2.1577545395743463
[Stage 38:>                                                         (0 + 2) / 2]Training loss at epoch 29: 2.1497513432975635
Training loss at epoch 30: 2.14245978904604
[Stage 40:>                                                         (0 + 2) / 2]Training loss at epoch 31: 2.1356748887424755
Training loss at epoch 32: 2.1295652695102123
[Stage 42:>                                                         (0 + 2) / 2]Training loss at epoch 33: 2.123813283278909
[Stage 43:>                                                         (0 + 2) / 2]Training loss at epoch 34: 2.1188126874370736
[Stage 44:=============================>                            (1 + 1) / 2]Training loss at epoch 35: 2.11277347338559
Training loss at epoch 36: 2.106164510981411
[Stage 46:=============================>                            (1 + 1) / 2]Training loss at epoch 37: 2.10046853387508
[Stage 47:>                                                         (0 + 2) / 2]Training loss at epoch 38: 2.094693236812745
[Stage 48:=============================>                            (1 + 1) / 2]Training loss at epoch 39: 2.0890843819932865
Training loss at epoch 40: 2.083788472225119
[Stage 50:>                                                         (0 + 2) / 2]Training loss at epoch 41: 2.0790655971645413
[Stage 51:>                                                         (0 + 2) / 2]Training loss at epoch 42: 2.0740047466525446
[Stage 52:=============================>                            (1 + 1) / 2]Training loss at epoch 43: 2.0684075217755815
[Stage 53:>                                                         (0 + 2) / 2]Training loss at epoch 44: 2.0714275651438725
[Stage 54:>                                                         (0 + 2) / 2]Training loss at epoch 45: 2.0652574323755672
Training loss at epoch 46: 2.067274932840355
[Stage 56:=============================>                            (1 + 1) / 2]Training loss at epoch 47: 2.0548095748796333
[Stage 57:>                                                         (0 + 2) / 2]Training loss at epoch 48: 2.0493727335695944
Training loss at epoch 49: 2.0563729575437706
[Stage 59:>                                                         (0 + 2) / 2]Training loss at epoch 50: 2.043645268752063
Training loss at epoch 51: 2.035804521991474
[Stage 61:>                                                         (0 + 2) / 2]Training loss at epoch 52: 2.033559469399445
Training loss at epoch 53: 2.033131230654314
Training loss at epoch 54: 2.0312697272607543
Training loss at epoch 55: 2.0317338260776765
[Stage 65:>                                                         (0 + 2) / 2]Training loss at epoch 56: 2.026531880370318
[Stage 66:>                                                         (0 + 2) / 2]Training loss at epoch 57: 2.021177355580952
[Stage 67:>                                                         (0 + 2) / 2]Training loss at epoch 58: 2.0184230145137088
[Stage 68:>                                                         (0 + 2) / 2]Training loss at epoch 59: 2.014320948773142
Training loss at epoch 60: 2.0137287836265596
Training loss at epoch 61: 2.0041813399313235
[Stage 71:=============================>                            (1 + 1) / 2]Training loss at epoch 62: 1.9972885055114271
Training loss at epoch 63: 1.9919244519629271
Training loss at epoch 64: 1.9874785542901419
[Stage 74:>                                                         (0 + 2) / 2]Training loss at epoch 65: 1.9838751198914806
[Stage 75:=============================>                            (1 + 1) / 2]Training loss at epoch 66: 1.978851346839331
[Stage 76:>                                                         (0 + 2) / 2]Training loss at epoch 67: 1.9746093689270416
Training loss at epoch 68: 1.9704398518965807
[Stage 78:>                                                         (0 + 2) / 2]Training loss at epoch 69: 1.9670986598647087
Training loss at epoch 70: 1.965893814636453
Training loss at epoch 71: 1.963318896637022
Training loss at epoch 72: 1.9621522409754983
Training loss at epoch 73: 1.957689240183559
[Stage 83:>                                                         (0 + 2) / 2]Training loss at epoch 74: 1.9563932454864255
[Stage 84:>                                                         (0 + 2) / 2]Training loss at epoch 75: 1.9544064416665545
Training loss at epoch 76: 1.9501062005863947
Training loss at epoch 77: 1.9487215405648048
[Stage 87:>                                                         (0 + 2) / 2]Training loss at epoch 78: 1.9478961944894768
Training loss at epoch 79: 1.9415626594726265
Training loss at epoch 80: 1.9377637411205304
[Stage 90:=============================>                            (1 + 1) / 2]Training loss at epoch 81: 1.9344990422113253
Training loss at epoch 82: 1.9312260541131754
[Stage 92:>                                                         (0 + 2) / 2]Training loss at epoch 83: 1.9271105722621766
[Stage 93:>                                                         (0 + 2) / 2]Training loss at epoch 84: 1.925125648990081
Training loss at epoch 85: 1.921164248031474
[Stage 95:=============================>                            (1 + 1) / 2]Training loss at epoch 86: 1.9180337893809305
Training loss at epoch 87: 1.9146604487070562
Training loss at epoch 88: 1.911597436846288
Training loss at epoch 89: 1.9087620941527648
[Stage 99:>                                                         (0 + 2) / 2]Training loss at epoch 90: 1.905696148251569
[Stage 100:>                                                        (0 + 2) / 2]Training loss at epoch 91: 1.9028793337865464
Training loss at epoch 92: 1.9004915883135531
[Stage 102:============================>                            (1 + 1) / 2]Training loss at epoch 93: 1.8982164025621215
[Stage 103:>                                                        (0 + 2) / 2]Training loss at epoch 94: 1.8957265655255022
[Stage 104:>                                                        (0 + 2) / 2]Training loss at epoch 95: 1.893395441615573
[Stage 105:>                                                        (0 + 2) / 2]Training loss at epoch 96: 1.9010229351364965
Training loss at epoch 97: 1.8938827878730824
Training loss at epoch 98: 1.8871330849509587
[Stage 108:============================>                            (1 + 1) / 2]Training loss at epoch 99: 1.8847075729697018
Training loss at epoch 100: 1.8826067388339283
[Stage 110:>                                                        (0 + 2) / 2]Training loss at epoch 101: 1.8825511067225857
[Stage 111:============================>                            (1 + 1) / 2]Training loss at epoch 102: 1.884881314287975
[Stage 112:============================>                            (1 + 1) / 2]Training loss at epoch 103: 1.8817527285632638
Training loss at epoch 104: 1.8782948032285982
Training loss at epoch 105: 1.8777017769867839
[Stage 115:============================>                            (1 + 1) / 2]Training loss at epoch 106: 1.8787732743471788
Training loss at epoch 107: 1.8720625682625893
Training loss at epoch 108: 1.8722650272567594
[Stage 118:============================>                            (1 + 1) / 2]Training loss at epoch 109: 1.8669938799770653
[Stage 119:============================>                            (1 + 1) / 2]Training loss at epoch 110: 1.8728023133035225
Training loss at epoch 111: 1.8648242884421888
[Stage 121:>                                                        (0 + 2) / 2]Training loss at epoch 112: 1.858919155468585
[Stage 122:>                                                        (0 + 2) / 2]Training loss at epoch 113: 1.85961802550318
Training loss at epoch 114: 1.8567404424509586
[Stage 124:>                                                        (0 + 2) / 2]Training loss at epoch 115: 1.8562348149126169
Training loss at epoch 116: 1.8498833372480468
[Stage 126:>                                                        (0 + 2) / 2]Training loss at epoch 117: 1.8471025058417365
[Stage 127:>                                                        (0 + 2) / 2]Training loss at epoch 118: 1.84890309836736
[Stage 128:============================>                            (1 + 1) / 2]Training loss at epoch 119: 1.8440552569324327
Training loss at epoch 120: 1.8522118811525692
Training loss at epoch 121: 1.8421798943604555
[Stage 131:>                                                        (0 + 2) / 2]Training loss at epoch 122: 1.836520318963801
Training loss at epoch 123: 1.8344796246429587
[Stage 133:>                                                        (0 + 2) / 2]Training loss at epoch 124: 1.8325775963064548
[Stage 134:>                                                        (0 + 2) / 2]Training loss at epoch 125: 1.8310645369024456
[Stage 135:============================>                            (1 + 1) / 2]Training loss at epoch 126: 1.8302676577884252
Training loss at epoch 127: 1.827119513934471
[Stage 137:>                                                        (0 + 2) / 2]Training loss at epoch 128: 1.8288959278132928
[Stage 138:>                                                        (0 + 2) / 2]Training loss at epoch 129: 1.828361856341649
[Stage 139:>                                                        (0 + 2) / 2]Training loss at epoch 130: 1.8257879032786546
Training loss at epoch 131: 1.822038514127811
[Stage 141:>                                                        (0 + 2) / 2]Training loss at epoch 132: 1.8194144330962276
[Stage 142:>                                                        (0 + 2) / 2]Training loss at epoch 133: 1.8178204169858578
Training loss at epoch 134: 1.8168121842690592
Training loss at epoch 135: 1.8124729887533308
[Stage 145:>                                                        (0 + 2) / 2]Training loss at epoch 136: 1.811135646100809
Training loss at epoch 137: 1.8111316691082786
[Stage 147:>                                                        (0 + 2) / 2]Training loss at epoch 138: 1.807429093488055
[Stage 148:>                                                        (0 + 2) / 2]Training loss at epoch 139: 1.8063584856599715
[Stage 149:============================>                            (1 + 1) / 2]Training loss at epoch 140: 1.8014421719082196
Training loss at epoch 141: 1.8024741533540332
[Stage 151:>                                                        (0 + 2) / 2]Training loss at epoch 142: 1.7991757098937082
[Stage 152:>                                                        (0 + 2) / 2]Training loss at epoch 143: 1.800082921182962
Training loss at epoch 144: 1.7974431156568704
[Stage 154:============================>                            (1 + 1) / 2]Training loss at epoch 145: 1.7972440177684739
Training loss at epoch 146: 1.7967693885518725
[Stage 156:>                                                        (0 + 2) / 2]Training loss at epoch 147: 1.793319236487993
Training loss at epoch 148: 1.793535686714872
[Stage 158:>                                                        (0 + 2) / 2]Training loss at epoch 149: 1.7899670373482541
Training loss at epoch 150: 1.7881325786702251
Training loss at epoch 151: 1.7881867359329868
[Stage 161:>                                                        (0 + 2) / 2]Training loss at epoch 152: 1.7862668868819778
Training loss at epoch 153: 1.7822660322930743
Training loss at epoch 154: 1.7817065945269048
[Stage 164:============================>                            (1 + 1) / 2]Training loss at epoch 155: 1.7815505183426819
Training loss at epoch 156: 1.778966317876122
[Stage 166:============================>                            (1 + 1) / 2]Training loss at epoch 157: 1.775940351500555
Training loss at epoch 158: 1.774702054589864
Training loss at epoch 159: 1.7746417629838773
Training loss at epoch 160: 1.77151944984415
Training loss at epoch 161: 1.7694627125788793
Training loss at epoch 162: 1.7669594703602622
[Stage 172:>                                                        (0 + 2) / 2]Training loss at epoch 163: 1.7647071437813517
Training loss at epoch 164: 1.7639210320740841
[Stage 174:>                                                        (0 + 2) / 2]Training loss at epoch 165: 1.760679461699498
[Stage 175:============================>                            (1 + 1) / 2]Training loss at epoch 166: 1.7587937559743454
Training loss at epoch 167: 1.7567450139122167
Training loss at epoch 168: 1.7544911099671705
[Stage 178:============================>                            (1 + 1) / 2]Training loss at epoch 169: 1.7531880259265222
Training loss at epoch 170: 1.7511998256203494
[Stage 180:>                                                        (0 + 2) / 2]Training loss at epoch 171: 1.7502238045512837
Training loss at epoch 172: 1.7484059489839041
[Stage 182:>                                                        (0 + 2) / 2]Training loss at epoch 173: 1.7471035221549625
Training loss at epoch 174: 1.7462545087184356
Training loss at epoch 175: 1.744913813118586
[Stage 185:>                                                        (0 + 2) / 2]Training loss at epoch 176: 1.7444777938789708
Training loss at epoch 177: 1.7437671069984004
Training loss at epoch 178: 1.7433888712426382
Training loss at epoch 179: 1.742826680218344
[Stage 189:============================>                            (1 + 1) / 2]Training loss at epoch 180: 1.7415342465490598
Training loss at epoch 181: 1.7392899840785259
[Stage 191:>                                                        (0 + 2) / 2]Training loss at epoch 182: 1.7367990479811368
[Stage 192:>                                                        (0 + 2) / 2]Training loss at epoch 183: 1.735061122981148
[Stage 193:>                                                        (0 + 2) / 2]Training loss at epoch 184: 1.7352220364466158
[Stage 194:>                                                        (0 + 2) / 2]Training loss at epoch 185: 1.7350677570687245
Training loss at epoch 186: 1.7338736618658486
[Stage 196:>                                                        (0 + 2) / 2]Training loss at epoch 187: 1.7326874059013588
[Stage 197:>                                                        (0 + 2) / 2]Training loss at epoch 188: 1.731319047520121
[Stage 198:>                                                        (0 + 2) / 2]Training loss at epoch 189: 1.7326892035390788
[Stage 199:>                                                        (0 + 2) / 2]Training loss at epoch 190: 1.7307598481460735
Training loss at epoch 191: 1.7309776279242837
[Stage 201:>                                                        (0 + 2) / 2]Training loss at epoch 192: 1.7282378773372216
Training loss at epoch 193: 1.7268782821773616
[Stage 203:>                                                        (0 + 2) / 2]Training loss at epoch 194: 1.7262271455439806
Training loss at epoch 195: 1.72495589349701
[Stage 205:>                                                        (0 + 2) / 2]Training loss at epoch 196: 1.7242076773926327
[Stage 206:>                                                        (0 + 2) / 2]Training loss at epoch 197: 1.723027156852115
Training loss at epoch 198: 1.7222608669852757
Training loss at epoch 199: 1.721256341409772
Training loss at epoch 200: 1.720637854457704
Training loss at epoch 201: 1.7199131272799422
[Stage 211:>                                                        (0 + 2) / 2]Training loss at epoch 202: 1.7190399190286563
[Stage 212:>                                                        (0 + 2) / 2]Training loss at epoch 203: 1.7176576344077523
Training loss at epoch 204: 1.716685782054849
[Stage 214:============================>                            (1 + 1) / 2]Training loss at epoch 205: 1.715786029902705
Training loss at epoch 206: 1.7142467733007367
[Stage 216:>                                                        (0 + 2) / 2]Training loss at epoch 207: 1.7148175164506843
[Stage 217:>                                                        (0 + 2) / 2]Training loss at epoch 208: 1.7139255101800295
[Stage 218:============================>                            (1 + 1) / 2]Training loss at epoch 209: 1.713152666037429
Training loss at epoch 210: 1.7115531202131604
[Stage 220:>                                                        (0 + 2) / 2]Training loss at epoch 211: 1.7093291129166914
[Stage 221:>                                                        (0 + 2) / 2]Training loss at epoch 212: 1.7090654814458464
[Stage 222:>                                                        (0 + 2) / 2]Training loss at epoch 213: 1.7080099099843626
Training loss at epoch 214: 1.7069781902406678
Training loss at epoch 215: 1.7062817052156443
[Stage 225:>                                                        (0 + 2) / 2]Training loss at epoch 216: 1.7059405499698281
[Stage 226:>                                                        (0 + 2) / 2]Training loss at epoch 217: 1.705779212153673
[Stage 227:>                                                        (0 + 2) / 2]Training loss at epoch 218: 1.7055101228102822
Training loss at epoch 219: 1.7057811781122487
[Stage 229:>                                                        (0 + 2) / 2]Training loss at epoch 220: 1.705257040883424
Training loss at epoch 221: 1.7047258801164022
Training loss at epoch 222: 1.7047886862017623
[Stage 232:>                                                        (0 + 2) / 2]Training loss at epoch 223: 1.70378789934972
[Stage 233:============================>                            (1 + 1) / 2]Training loss at epoch 224: 1.7022328091444932
[Stage 234:============================>                            (1 + 1) / 2]Training loss at epoch 225: 1.7016487648181888
[Stage 235:============================>                            (1 + 1) / 2]Training loss at epoch 226: 1.7015439809427109
Training loss at epoch 227: 1.7022913272485238
Training loss at epoch 228: 1.7006785715827446
Training loss at epoch 229: 1.7012511460662698
Training loss at epoch 230: 1.7012734789102781
Training loss at epoch 231: 1.7021373599329002
[Stage 241:============================>                            (1 + 1) / 2]Training loss at epoch 232: 1.7046413891106362
[Stage 242:============================>                            (1 + 1) / 2]Training loss at epoch 233: 1.699895217032163
[Stage 243:>                                                        (0 + 2) / 2]Training loss at epoch 234: 1.6992386289060188
[Stage 244:============================>                            (1 + 1) / 2]Training loss at epoch 235: 1.697250148349248
[Stage 245:>                                                        (0 + 2) / 2]Training loss at epoch 236: 1.6969125558384222
[Stage 246:>                                                        (0 + 2) / 2]Training loss at epoch 237: 1.6947327123936935
[Stage 247:>                                                        (0 + 2) / 2]Training loss at epoch 238: 1.6962555792373213
[Stage 248:>                                                        (0 + 2) / 2]Training loss at epoch 239: 1.6922588884942926
Training loss at epoch 240: 1.6924631363093727
[Stage 250:>                                                        (0 + 2) / 2]Training loss at epoch 241: 1.691049396500098
Training loss at epoch 242: 1.6910877029577747
[Stage 252:>                                                        (0 + 2) / 2]Training loss at epoch 243: 1.6917949009301516
Training loss at epoch 244: 1.6907831374016098
Training loss at epoch 245: 1.68793310192313
[Stage 255:>                                                        (0 + 2) / 2]Training loss at epoch 246: 1.6860083620978148
Training loss at epoch 247: 1.685153576445032
[Stage 257:>                                                        (0 + 2) / 2]Training loss at epoch 248: 1.6847886787588109
[Stage 258:>                                                        (0 + 2) / 2]Training loss at epoch 249: 1.6849844283658704
[Stage 259:>                                                        (0 + 2) / 2]Training loss at epoch 250: 1.6847329280252414
Training loss at epoch 251: 1.6834401879862886
[Stage 261:============================>                            (1 + 1) / 2]Training loss at epoch 252: 1.6830480856637653
[Stage 262:============================>                            (1 + 1) / 2]Training loss at epoch 253: 1.6830161089598454
Training loss at epoch 254: 1.6823828849959501
Training loss at epoch 255: 1.6819754636969493
[Stage 265:>                                                        (0 + 2) / 2]Training loss at epoch 256: 1.6809359449124535
[Stage 266:>                                                        (0 + 2) / 2]Training loss at epoch 257: 1.6801673502450725
[Stage 267:>                                                        (0 + 2) / 2]Training loss at epoch 258: 1.679129447971881
Training loss at epoch 259: 1.678151468487294
[Stage 269:============================>                            (1 + 1) / 2]Training loss at epoch 260: 1.676383439247174
Training loss at epoch 261: 1.6752731812747705
Training loss at epoch 262: 1.6741645158632943
[Stage 272:>                                                        (0 + 2) / 2]Training loss at epoch 263: 1.6718489735608157
[Stage 273:>                                                        (0 + 2) / 2]Training loss at epoch 264: 1.6697753175312458
[Stage 274:============================>                            (1 + 1) / 2]Training loss at epoch 265: 1.6703913732389004
[Stage 275:>                                                        (0 + 2) / 2]Training loss at epoch 266: 1.6701693834248772
[Stage 276:>                                                        (0 + 2) / 2]Training loss at epoch 267: 1.6688164682592925
Training loss at epoch 268: 1.6682142967363323
[Stage 278:>                                                        (0 + 2) / 2]Training loss at epoch 269: 1.667526940967369
[Stage 279:>                                                        (0 + 2) / 2]Training loss at epoch 270: 1.6670970802609386
[Stage 280:>                                                        (0 + 2) / 2]Training loss at epoch 271: 1.6670811343996572
Training loss at epoch 272: 1.667360630588666
[Stage 282:>                                                        (0 + 2) / 2]Training loss at epoch 273: 1.665658980756813
Training loss at epoch 274: 1.6640660350307177
Training loss at epoch 275: 1.663090584639762
[Stage 285:>                                                        (0 + 2) / 2]Training loss at epoch 276: 1.6630665480996396
[Stage 286:>                                                        (0 + 2) / 2]Training loss at epoch 277: 1.6605579406854087
Training loss at epoch 278: 1.6579588132424996
Training loss at epoch 279: 1.6573928034386196
Training loss at epoch 280: 1.657010089528821
Training loss at epoch 281: 1.6554108165648895
Training loss at epoch 282: 1.6539592372942038
Training loss at epoch 283: 1.652945166392946
Training loss at epoch 284: 1.6509816284316192
[Stage 294:============================>                            (1 + 1) / 2]Training loss at epoch 285: 1.6486490738580286
Training loss at epoch 286: 1.6473754129681457
Training loss at epoch 287: 1.6451214838718828
Training loss at epoch 288: 1.6441394885856206
[Stage 298:>                                                        (0 + 2) / 2]Training loss at epoch 289: 1.644092262420172
Training loss at epoch 290: 1.642952929422399
[Stage 300:>                                                        (0 + 2) / 2]Training loss at epoch 291: 1.6409405839506979
[Stage 301:============================>                            (1 + 1) / 2]Training loss at epoch 292: 1.6393117544315479
[Stage 302:>                                                        (0 + 2) / 2]Training loss at epoch 293: 1.6387129155312743
[Stage 303:>                                                        (0 + 2) / 2]Training loss at epoch 294: 1.638385119602633
[Stage 304:>                                                        (0 + 2) / 2]Training loss at epoch 295: 1.6377860926196157
[Stage 305:>                                                        (0 + 2) / 2]Training loss at epoch 296: 1.63699285835549
Training loss at epoch 297: 1.6362990045769736
Training loss at epoch 298: 1.6360171933232088
[Stage 308:>                                                        (0 + 2) / 2]Training loss at epoch 299: 1.6359885711637268
[Stage 309:============================>                            (1 + 1) / 2]Training loss at epoch 300: 1.6351535993300776
[Stage 310:============================>                            (1 + 1) / 2]Training loss at epoch 301: 1.6336944225207846
[Stage 311:============================>                            (1 + 1) / 2]Training loss at epoch 302: 1.633379659556454
Training loss at epoch 303: 1.6313150746959761
[Stage 313:============================>                            (1 + 1) / 2]Training loss at epoch 304: 1.6306821203245456
[Stage 314:============================>                            (1 + 1) / 2]Training loss at epoch 305: 1.6303103379992867
Exception in thread "main" org.apache.spark.SparkException: Job 311 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1581)
	at org.apache.spark.SparkContext$$anonfun$stop$9.apply$mcV$sp(SparkContext.scala:1740)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1739)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.shallowNN.char_RNN_dist_para$char_RNN.fit(char_RNN_dist_para.scala:392)
	at org.apache.spark.shallowNN.char_RNN_dist_para$.main(char_RNN_dist_para.scala:450)
	at org.apache.spark.shallowNN.char_RNN_dist_para.main(char_RNN_dist_para.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
16/05/31 23:47:33 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@650125c5)
16/05/31 23:47:33 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(311,1464763653576,JobFailed(org.apache.spark.SparkException: Job 311 cancelled because SparkContext was shut down))

Process finished with exit code 130

