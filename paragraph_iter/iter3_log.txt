/usr/lib/jvm/java-8-oracle/bin/java -Dspark.master=local[2] -Didea.launcher.port=7532 -Didea.launcher.bin.path=/home/tblee/idea-IC/bin -Dfile.encoding=UTF-8 -classpath "/usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/deploy.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/cldrdata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/dnsns.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jaccess.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/jfxrt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/localedata.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/nashorn.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunec.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunjce_provider.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/sunpkcs11.jar:/usr/lib/jvm/java-8-oracle/jre/lib/ext/zipfs.jar:/usr/lib/jvm/java-8-oracle/jre/lib/javaws.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfxswt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/management-agent.jar:/usr/lib/jvm/java-8-oracle/jre/lib/plugin.jar:/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/media/tblee/Data/Stanford courses/Spring 2016/CME323/Project/rnn_nlp/target/scala-2.11/classes:/home/tblee/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.11.8.jar:/home/tblee/spark-1.6.1/assembly/target/scala-2.11/spark-assembly-1.6.1-hadoop2.4.0.jar:/home/tblee/idea-IC/lib/idea_rt.jar" com.intellij.rt.execution.application.AppMain org.apache.spark.shallowNN.char_RNN_dist_para
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel("INFO")
16/06/01 00:04:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/06/01 00:04:55 WARN Utils: Your hostname, tblee-UX303LB resolves to a loopback address: 127.0.1.1; using 10.0.0.6 instead (on interface wlan0)
16/06/01 00:04:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Input data has vocabulary size 66, initializing network with 1 layers each has 100 hidden units, training batch size 25
16/06/01 00:05:00 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
16/06/01 00:05:00 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
Training loss at epoch 0: 6.519171850443139
Training loss at epoch 1: 3.019437879229976
Training loss at epoch 2: 2.7723989382461296
Training loss at epoch 3: 2.5853213140978846
Training loss at epoch 4: 2.4656182838052905
[Stage 14:=============================>                            (1 + 1) / 2]Training loss at epoch 5: 2.388810918721182
[Stage 15:=============================>                            (1 + 1) / 2]Training loss at epoch 6: 2.332829032726246
Training loss at epoch 7: 2.2923222140969655
[Stage 17:=============================>                            (1 + 1) / 2]Training loss at epoch 8: 2.254576872319157
[Stage 18:=============================>                            (1 + 1) / 2]Training loss at epoch 9: 2.2224342047381263
[Stage 19:=============================>                            (1 + 1) / 2]Training loss at epoch 10: 2.1978863580622114
[Stage 20:=============================>                            (1 + 1) / 2]Training loss at epoch 11: 2.1729035404498847
Training loss at epoch 12: 2.15201505771635
Training loss at epoch 13: 2.1337020174643855
Training loss at epoch 14: 2.116788737219074
[Stage 24:>                                                         (0 + 2) / 2]Training loss at epoch 15: 2.1018632162168953
Training loss at epoch 16: 2.0887556807200918
[Stage 26:=============================>                            (1 + 1) / 2]Training loss at epoch 17: 2.076308735039642
[Stage 27:=============================>                            (1 + 1) / 2]Training loss at epoch 18: 2.0651501323845345
[Stage 28:=============================>                            (1 + 1) / 2]Training loss at epoch 19: 2.053579473779451
[Stage 29:=============================>                            (1 + 1) / 2]Training loss at epoch 20: 2.0436323014658675
[Stage 30:=============================>                            (1 + 1) / 2]Training loss at epoch 21: 2.033502655716802
[Stage 31:=============================>                            (1 + 1) / 2]Training loss at epoch 22: 2.0240681984021047
[Stage 32:=============================>                            (1 + 1) / 2]Training loss at epoch 23: 2.0142242641433215
[Stage 33:=============================>                            (1 + 1) / 2]Training loss at epoch 24: 2.004930604542352
[Stage 34:=============================>                            (1 + 1) / 2]Training loss at epoch 25: 1.9940708382477257
[Stage 35:=============================>                            (1 + 1) / 2]Training loss at epoch 26: 1.9842336531867872
[Stage 36:=============================>                            (1 + 1) / 2]Training loss at epoch 27: 1.9747023091839309
[Stage 37:=============================>                            (1 + 1) / 2]Training loss at epoch 28: 1.9661651159895996
[Stage 38:=============================>                            (1 + 1) / 2]Training loss at epoch 29: 1.958155453957304
[Stage 39:=============================>                            (1 + 1) / 2]Training loss at epoch 30: 1.9486964653906864
Training loss at epoch 31: 1.9389698027669726
[Stage 41:=============================>                            (1 + 1) / 2]Training loss at epoch 32: 1.9293727789768236
[Stage 42:=============================>                            (1 + 1) / 2]Training loss at epoch 33: 1.9207218420988315
[Stage 43:=============================>                            (1 + 1) / 2]Training loss at epoch 34: 1.9135087374586583
[Stage 44:=============================>                            (1 + 1) / 2]Training loss at epoch 35: 1.9056762025700413
Training loss at epoch 36: 1.898978743125461
Training loss at epoch 37: 1.891687794104755
[Stage 47:>                                                         (0 + 2) / 2]Training loss at epoch 38: 1.8848910288864524
[Stage 48:=============================>                            (1 + 1) / 2]Training loss at epoch 39: 1.8783414693523157
Training loss at epoch 40: 1.871080715905002
Training loss at epoch 41: 1.868243857264007
Training loss at epoch 42: 1.8570834418305786
[Stage 52:=============================>                            (1 + 1) / 2]Training loss at epoch 43: 1.8523898252093065
Training loss at epoch 44: 1.8462049931501765
[Stage 54:=============================>                            (1 + 1) / 2]Training loss at epoch 45: 1.8402492108204558
Training loss at epoch 46: 1.8359937764865357
Training loss at epoch 47: 1.8295935045072431
Training loss at epoch 48: 1.8244761068446491
[Stage 58:=============================>                            (1 + 1) / 2]Training loss at epoch 49: 1.8168469322401117
Training loss at epoch 50: 1.809643728325062
[Stage 60:=============================>                            (1 + 1) / 2]Training loss at epoch 51: 1.8043324669323053
[Stage 61:=============================>                            (1 + 1) / 2]Training loss at epoch 52: 1.7999988960822892
[Stage 62:=============================>                            (1 + 1) / 2]Training loss at epoch 53: 1.7967451926940516
[Stage 63:>                                                         (0 + 2) / 2]Training loss at epoch 54: 1.7900503019189735
[Stage 64:=============================>                            (1 + 1) / 2]Training loss at epoch 55: 1.785588367128149
Training loss at epoch 56: 1.7816662252528601
[Stage 66:=============================>                            (1 + 1) / 2]Training loss at epoch 57: 1.7782434286731768
Training loss at epoch 58: 1.771563852202241
Training loss at epoch 59: 1.767010698767607
[Stage 69:=============================>                            (1 + 1) / 2]Training loss at epoch 60: 1.7621053446592996
[Stage 70:=============================>                            (1 + 1) / 2]Training loss at epoch 61: 1.7573104019643409
[Stage 71:=============================>                            (1 + 1) / 2]Training loss at epoch 62: 1.7529639037262292
Training loss at epoch 63: 1.7495128141641343
[Stage 73:=============================>                            (1 + 1) / 2]Training loss at epoch 64: 1.745132442379624
Training loss at epoch 65: 1.7412778874650194
Training loss at epoch 66: 1.7374900406558302
[Stage 76:=============================>                            (1 + 1) / 2]Training loss at epoch 67: 1.7341832910177175
[Stage 77:=============================>                            (1 + 1) / 2]Training loss at epoch 68: 1.7312037268967655
[Stage 78:=============================>                            (1 + 1) / 2]Training loss at epoch 69: 1.727399646997538
[Stage 79:=============================>                            (1 + 1) / 2]Training loss at epoch 70: 1.7245893849049423
[Stage 80:>                                                         (0 + 2) / 2]Training loss at epoch 71: 1.721374316346236
[Stage 81:=============================>                            (1 + 1) / 2]Training loss at epoch 72: 1.7179595526399452
[Stage 82:>                                                         (0 + 2) / 2]Training loss at epoch 73: 1.713702742662437
Training loss at epoch 74: 1.710293452270959
[Stage 84:=============================>                            (1 + 1) / 2]Training loss at epoch 75: 1.7065644835979108
[Stage 85:=============================>                            (1 + 1) / 2]Training loss at epoch 76: 1.7029163398118514
[Stage 86:=============================>                            (1 + 1) / 2]Training loss at epoch 77: 1.6996935372751936
Training loss at epoch 78: 1.6963058579672665
[Stage 88:=============================>                            (1 + 1) / 2]Training loss at epoch 79: 1.6926619333538617
[Stage 89:=============================>                            (1 + 1) / 2]Training loss at epoch 80: 1.6890511020016825
Training loss at epoch 81: 1.6852688239646505
[Stage 91:=============================>                            (1 + 1) / 2]Training loss at epoch 82: 1.6817409750320285
[Stage 92:>                                                         (0 + 2) / 2]Training loss at epoch 83: 1.6794491615460174
Training loss at epoch 84: 1.6764341607152689
Training loss at epoch 85: 1.6736306668747776
[Stage 95:=============================>                            (1 + 1) / 2]Training loss at epoch 86: 1.6710008701785506
[Stage 96:>                                                         (0 + 2) / 2]Training loss at epoch 87: 1.6683149991019184
[Stage 97:=============================>                            (1 + 1) / 2]Training loss at epoch 88: 1.6650297911084488
[Stage 98:=============================>                            (1 + 1) / 2]Training loss at epoch 89: 1.6624750929229055
[Stage 99:=============================>                            (1 + 1) / 2]Training loss at epoch 90: 1.6602057596472557
[Stage 100:============================>                            (1 + 1) / 2]Training loss at epoch 91: 1.6579289857538209
[Stage 101:============================>                            (1 + 1) / 2]Training loss at epoch 92: 1.654806626141884
[Stage 102:============================>                            (1 + 1) / 2]Training loss at epoch 93: 1.6515090906044578
[Stage 103:============================>                            (1 + 1) / 2]Training loss at epoch 94: 1.6480832113195614
[Stage 104:============================>                            (1 + 1) / 2]Training loss at epoch 95: 1.6448741273388683
[Stage 105:============================>                            (1 + 1) / 2]Training loss at epoch 96: 1.6411164980394441
[Stage 106:>                                                        (0 + 2) / 2]Training loss at epoch 97: 1.6374513995006765
Training loss at epoch 98: 1.6333967744609892
[Stage 108:>                                                        (0 + 2) / 2]Training loss at epoch 99: 1.6307813514269176
[Stage 109:============================>                            (1 + 1) / 2]Training loss at epoch 100: 1.6271657857499973
Training loss at epoch 101: 1.62384146646102
[Stage 111:============================>                            (1 + 1) / 2]Training loss at epoch 102: 1.620496995872965
Training loss at epoch 103: 1.617572279523407
[Stage 113:============================>                            (1 + 1) / 2]Training loss at epoch 104: 1.6145586302662396
[Stage 114:============================>                            (1 + 1) / 2]Training loss at epoch 105: 1.6116985114911797
[Stage 115:============================>                            (1 + 1) / 2]Training loss at epoch 106: 1.6084072005389407
Training loss at epoch 107: 1.6053160624792326
[Stage 117:>                                                        (0 + 2) / 2]Training loss at epoch 108: 1.6021969317206444
Training loss at epoch 109: 1.5993575841530512
Training loss at epoch 110: 1.5964758630687244
[Stage 120:============================>                            (1 + 1) / 2]Training loss at epoch 111: 1.5939791145285376
[Stage 121:============================>                            (1 + 1) / 2]Training loss at epoch 112: 1.5913993855412485
Training loss at epoch 113: 1.5894673979638196
[Stage 123:>                                                        (0 + 2) / 2]Training loss at epoch 114: 1.5866225124746443
[Stage 124:>                                                        (0 + 2) / 2]Training loss at epoch 115: 1.5840683414978554
Training loss at epoch 116: 1.5811137155577903
Training loss at epoch 117: 1.5789603456691697
[Stage 127:============================>                            (1 + 1) / 2]Training loss at epoch 118: 1.5768191638479088
Training loss at epoch 119: 1.5748922478359466
Training loss at epoch 120: 1.5724074036820206
[Stage 130:============================>                            (1 + 1) / 2]Training loss at epoch 121: 1.5706572052969636
[Stage 131:============================>                            (1 + 1) / 2]Training loss at epoch 122: 1.5687775910036725
[Stage 132:============================>                            (1 + 1) / 2]Training loss at epoch 123: 1.5658970126184684
[Stage 133:============================>                            (1 + 1) / 2]Training loss at epoch 124: 1.5631008564118842
[Stage 134:============================>                            (1 + 1) / 2]Training loss at epoch 125: 1.5606278837097403
[Stage 135:============================>                            (1 + 1) / 2]Training loss at epoch 126: 1.5590372357464441
[Stage 136:============================>                            (1 + 1) / 2]Training loss at epoch 127: 1.5569608430024249
Training loss at epoch 128: 1.5549880019304274
[Stage 138:============================>                            (1 + 1) / 2]Training loss at epoch 129: 1.5535205546672597
[Stage 139:============================>                            (1 + 1) / 2]Training loss at epoch 130: 1.5512878979275433
Training loss at epoch 131: 1.5491324748006707
[Stage 141:============================>                            (1 + 1) / 2]Training loss at epoch 132: 1.5478137207852412
[Stage 142:============================>                            (1 + 1) / 2]Training loss at epoch 133: 1.5465143097179386
[Stage 143:>                                                        (0 + 2) / 2]Training loss at epoch 134: 1.5443647027144998
[Stage 144:============================>                            (1 + 1) / 2]Training loss at epoch 135: 1.5423999462596312
Training loss at epoch 136: 1.5404342507728397
[Stage 146:============================>                            (1 + 1) / 2]Training loss at epoch 137: 1.538005937410021
Training loss at epoch 138: 1.5375028075331405
[Stage 148:============================>                            (1 + 1) / 2]Training loss at epoch 139: 1.5346098494932154
[Stage 149:============================>                            (1 + 1) / 2]Training loss at epoch 140: 1.533059057568397
Training loss at epoch 141: 1.5310514464529683
[Stage 151:============================>                            (1 + 1) / 2]Training loss at epoch 142: 1.5287648976258466
[Stage 152:>                                                        (0 + 2) / 2]Exception in thread "main" org.apache.spark.SparkException: Job 148 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1581)
	at org.apache.spark.SparkContext$$anonfun$stop$9.apply$mcV$sp(SparkContext.scala:1740)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1739)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.shallowNN.char_RNN_dist_para$char_RNN.fit(char_RNN_dist_para.scala:392)
	at org.apache.spark.shallowNN.char_RNN_dist_para$.main(char_RNN_dist_para.scala:450)
	at org.apache.spark.shallowNN.char_RNN_dist_para.main(char_RNN_dist_para.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
16/06/01 00:12:47 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@355a5441)
16/06/01 00:12:47 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(148,1464765167679,JobFailed(org.apache.spark.SparkException: Job 148 cancelled because SparkContext was shut down))

Process finished with exit code 130

